# Импорт библиотеки для численных вычислений
import numpy as np


# Определение целевой функции для оптимизации
def f(x):
    """Целевая функция: f(x) = x1^4 - 2x1x2 + x2^4 - x1^2 - x2^2"""
    # Разделяем вектор x на компоненты
    x1, x2 = x
    # Вычисляем значение функции по заданной формуле
    return x1 ** 4 - 2 * x1 * x2 + x2 ** 4 - x1 ** 2 - x2 ** 2


# Функция вычисления градиента (вектора первых производных)
def gradient(x):
    """Градиент функции"""
    # Разделяем вектор x на компоненты
    x1, x2 = x
    # Вычисляем частную производную по x1
    df_dx1 = 4 * x1 ** 3 - 2 * x2 - 2 * x1
    # Вычисляем частную производную по x2
    df_dx2 = -2 * x1 + 4 * x2 ** 3 - 2 * x2
    # Возвращаем градиент как numpy-массив
    return np.array([df_dx1, df_dx2])


# Функция вычисления матрицы Гессе (матрицы вторых производных)
def hessian(x):
    """Матрица Гессе"""
    # Разделяем вектор x на компоненты
    x1, x2 = x
    # Вычисляем вторую производную по x1 дважды
    d2f_dx1_2 = 12 * x1 ** 2 - 2
    # Вычисляем вторую производную по x2 дважды
    d2f_dx2_2 = 12 * x2 ** 2 - 2
    # Вычисляем смешанную производную (одинакова для обоих порядков дифференцирования)
    d2f_dx1_dx2 = -2
    # Возвращаем матрицу Гессе размером 2x2
    return np.array([[d2f_dx1_2, d2f_dx1_dx2], [d2f_dx1_dx2, d2f_dx2_2]])


# Реализация метода Ньютона для оптимизации
def newton_method(x0, eps1=1e-6, eps2=1e-6, max_iter=100):
    """
    Классический метод Ньютона с фиксированным шагом t=1.

    Параметры:
    x0 - начальная точка
    eps1 - критерий остановки по градиенту
    eps2 - критерий остановки по изменению x
    max_iter - максимальное число итераций
    """
    # Создаем копию начальной точки
    x = x0.copy()
    # Счетчик итераций
    k = 0

    # Основной цикл оптимизации
    for k in range(max_iter):
        # Вычисляем градиент в текущей точке
        grad = gradient(x)

        # Проверка критерия остановки по норме градиента
        if np.linalg.norm(grad) < eps1:
            print(f"Критерий по градиенту выполнен на итерации {k}")
            return x, f(x)

        try:
            # Вычисляем матрицу Гессе
            H = hessian(x)
            # Пытаемся найти обратную матрицу
            H_inv = np.linalg.inv(H)
            # Вычисляем направление Ньютона
            d = -H_inv @ grad
        except np.linalg.LinAlgError:
            # Если матрица вырождена (необратима)
            print("Матрица Гессе вырождена. Остановка.")
            return x, f(x)

        # Вычисляем новую точку с фиксированным шагом t=1
        x_new = x + d

        # Проверка критерия остановки по изменению точки
        if np.linalg.norm(x_new - x) < eps2:
            print(f"Критерий по изменению x выполнен на итерации {k}")
            return x_new, f(x_new)

        # Обновляем текущую точку для следующей итерации
        x = x_new

    # Если достигнуто максимальное число итераций
    print(f"Достигнуто максимальное число итераций {max_iter}")
    return x, f(x)


# Начальные параметры алгоритма
x0 = np.array([2, 2])  # Начальная точка
eps1 = 1e-3  # Точность по градиенту
eps2 = 1e-3  # Точность по изменению x

# Запуск метода Ньютона
x_opt, f_opt = newton_method(x0, eps1, eps2)

# Вывод результатов
print("\nРезультаты:")
print(f"Точка минимума: x = {x_opt}")
print(f"Значение функции: f(x) = {f_opt}")
print(f"Градиент в точке: {gradient(x_opt)}")
print(f"Собственные значения Гессе: {np.linalg.norm(hessian(x_opt))}")
